% |||||||||||||||||||||||||||||||||||
% |||||| 5.2 Dynamic modelling ||||||
% |||||||||||||||||||||||||||||||||||

% -----------------------------------------
% labels: \label{[type]:PT:dynamics:[name]}
% -----------------------------------------











% When translating a theory consisting of nice, continuous functions with theoretical limits to a discrete, numerical system, there are \blahblah. When said theory also includes a phase transition, it does not become any easier. Phase transitions are manifestly computational headaches, and we are limited by \blahblah. The discontinuity introduced by the phase transition is even more complicated to replicate in simulations. 




% \deleteme{
% The list of assumptions and terms starting with ``quasi'' has become uncomfortably long. Most of these have been tested before or are otherwise motivated by similar established phenomena in Minkowski space. %the ``Why not?''-aspect. 
% We want to test how the thin-wall NG dynamics associates with the field-theoretically formulated dynamics of a cosmic phase transition, explicitly in a generic spacetime. Cosmological contexts seldom care about spacetimes other than FLRW. The effect of curvature is insignificant in the Friedmann equations, thus irrelevant in \emph{our} universe, and we can safely assume (conformal) flatness. 
% }





As test bed for our theory we use the \texttt{C++}-code \asgrd~\citep{christiansenGravitationalWavesDark2024};\footnote{Code available on~\url{https://github.com/oyvach/AsGRD} (November 2024).} an extended version of the massively parallelised relativistic $N$-body code \gevolution~\citep{adamekGeneralRelativityCosmic2016}. \latfield~\citep{daverioLatfield2LibraryClassical2016} is used as backend to this. % and was originally \blahblah
We will describe the code in broad terms, and refer to~\citet{christiansenAsevolutionRelativisticNbody2023,christiansenAsimulationDomainFormation2024,adamekGevolutionCosmologicalNbody2016,christiansenGravitationalWavesDark2024,daverioLatfield2LibraryClassical2016} for more detailed explanations.

% The particle-mesh code \gevolution{} is
% It is parallelised in accordance with the MPI standard.   

% \comment{Write about code!}





% \speak{Section about pre-written code}


\subsubsection{Prelude}

The general setup is a three-dimensional box of equal side lengths $L_\#$ in $\Mpch$ on a lattice of $N_\#^3$ points, giving a spatial resolution of $\Delta_\#=L_\#/N_\#$. %We refer to the points in the configuration space as 
A Cartesian coordinate system defines the comoving coordinate $\vec{x}=(x,y,z)$ and wave vector $\vec{k}=(k_x, k_y, k_z)$. The mapping from lattice coordinates $\lcoordx= (\lcoord{i}, \lcoord{j}, \lcoord{k})$ to comoving is $\vec{x}=\lcoordx\cdot  \Delta_\#$, where $\lcoord{i},\lcoord{j}, \lcoord{k}\in [0, N_\#-1]\in \Integer$. 
Correspondingly in Fourier space, we have lattice coordinates $\lcoordk =(\lcoord{u}, \lcoord{v}, \lcoord{w})$, where $\lcoord{u}, \lcoord{v}, \lcoord{w} \in\pm[0, N_\#/2-1]$, and the mapping $\vec{k} =\lcoordk \cdot k_\# $. Here,  $k_\#=2\ppi/L_\#$ is the fundamental frequency. 
The box's boundaries are \emph{periodic} which means that for any function $f(\lcoordx)$, we have
% $f(\lcoord{i}, \lcoord{j}, \lcoord{k}) = f(\lcoord{i}+N_1 L_\# +\lcoord{i}+N_2 L_\#  + \lcoord{k}+N_3 L_\# )$ for $N_i \in \Integer$.
\begin{equation}\label{eq:PT:code:periodic_boundaries}
    f(\lcoord{i}, \lcoord{j}, \lcoord{k}) = f(\lcoord{i}+N_1 L_\# +\lcoord{i}+N_2 L_\#  + \lcoord{k}+N_3 L_\# ) \quad\forall \,  N_i \in \Integer.
\end{equation}
Furthermore, the box is decomposed in rod domains for MPI-parallelisation.
% \begin{equation}
%     \lcoordx = \lcoordx + 
% \end{equation}

% \phpar[about periodic boundaries---Dirac comb: $\Sha_{L_\#}(z) = \frac{1}{L_{\#}}  \Sha (z/L_\#)$]

The code automatically outputs scalar quantities like extremal values of $\chi$ and $q$, as well as their box-averaged values. Besides this, we request two-dimensional snapshots of the field in question at every-something time step. The file type for the snapshots are Hierarchical Data Format (HDF5). Saving this data is a computational cost, and requires available memory. %The Hierarchical Data Format (HDF5) 

% Choosing what to output and when is a matter of 

% We cannot save all data  %The user needs to prior



% \subsection{Periodic boundaries}
%     The simulation box is a torus as it has periodic boundaries---a common choice for cosmological simulations. 

%     \begin{bullets}
%         \item Dirac comb: $\Sha_{L_\#}(z) = \frac{1}{L_{\#}}  \Sha (z/L_\#)$
%     \end{bullets}


\paragraph{Fast Fourier transforms.} %
We write $f(\lcoordx)$ and its discrete Fourier transform (DFT) $\tilde{f}(\lcoordk)$ as
\begin{equation}\label{eq:PT:gwas:DFT_definition}
    f(\lcoordx) = \frac{1}{N_\#^3} \sum_{\lcoordk } \eu[-2\ppi \im \lcoordk \lcoordx/N_\# ]\tilde{f}(\lcoordk) , \quad \tilde{f}(\lcoordk) = \sum_{\lcoordx} \eu[+2\ppi \im \lcoordk \lcoordx/N_\#  ]f(\lcoordx).
\end{equation}
The equations are solved using fast Fourier transforms (FFTs), taken care of by the~\latfield{} library~\citep{daverioLatfield2LibraryClassical2016}. Hermitian symmetry gives
\begin{equation}
    \tilde{f}(-\lcoordk) =  \tilde{f}^\ast(\lcoordk),
\end{equation}
where $\tilde{f}^\ast$ is the complex conjugate. 


\paragraph{Units and conventions.} %
The code measures length in units of $L_\#$ and the DFT sign convention is opposite of~\cref{eq:PT:gwas:DFT_definition} \emph{and} without normalisation. We need to adjust for this when comparing results to calculations, e.g.~$\tilde{f}\rvert\ped{calc.} \leftrightarrow \tilde{f}^\ast\rvert\ped{code}$.





\subsection{Equations}

    Perturbations to the FLRW background are evolved in accordance with the linearised Einstein equation.\footnote{For details, see~\citet{adamekGevolutionCosmologicalNbody2016}.} %
    The tensorial part obeys 
    % For the tensorial part, this is %
    \begin{equation}
        % \ddot{h}\_{ij} + 2\mathcal{H} \dot{h}\_{ij} - \vec{\nabla}^2 h\_{ij} = 2\pclosed{S\_{ij}-\frac{1}{3}\Krondelta{_{ij}} \Krondelta{^{kl}}S\_{kl} }.%16\ppi G\nped{N}  \ProjectionLambda{ij}{kl}T\_{kl}
        \ddot{\Ft{h}}\_{ij} + 2\mathcal{H} \dot{\Ft{h}}\_{ij} + k^2 \Ft{h}\_{ij} =  2M\nped{Pl}^{-2} \Ft{\Pi}\indices*{^{\mathrm{TT}}_{ij}}
    \end{equation}
    as elaborated in~\cref{sec:GR:gws:gws_FLRW}. % 
    The asymmetron field is evolved in the Einstein frame with the equation
    \begin{equation}
        \dot{q} = \vec{\nabla}^2 \chi - a^2 \mu^2 \cclosed{\chi^3 - 2\bar{\kappa} \chi^2 + (\upsilon-1) \chi},
    \end{equation}
    where $q\equiv a^2 \dot{\chi}=a^2 \dv*{\chi}{\tau}$ is the ``speed'' of the field. As before (\cref{sec:cosmo:quintessence:asymmetron}), $\mu$ and $\bar{\kappa}=\kappa /(2\mu\sqrt{\lambda})$ encode the asymmetron parameters. 

    % The discretised versions of these are \blahblah


    \asgrd{} solves discretised versions of these via a numerical-integration protocol chosen by the user. The fourth-order Runge-Kutta scheme and Leap-frog method are amongst the options. %See~\citep{adamekGevolutionCosmologicalNbody2016} for more details. 
    The size of the time step $\Delta_\tau^{(\psi)}$ for the solver of the field $\psi$ is given in terms of the Courant factor~\citep{christiansenAsimulationDomainFormation2024}
    \begin{equation}
        C\ped{f}^{(\psi)} = v\ped{g}^{(\psi)} \frac{\Delta_\tau^{(\psi)}}{\Delta_\#},
    \end{equation}
    where $ v\ped{g}^{(\psi)}$ is the group velocity of said field. $\psi$ here represents CDM or $\phi$, and we use $v\ped{g}\ap{(cdm)}\sim 0.02$ and $v\ped{g}^{(\phi)}= 1$. \comment{Maybe just skip this...}








    % \comment{Text about FFTs}


    % \subsection{Parallelisation}
    % \latfield{} is MPI-parallelised in with a rod domain decomposition. 



    % \deleteme{
    % \subsection*{High-performance computing}
    % It is utterly unpractical to perform high-resolution relativistic $N$-body simulations without techniques from high-performance computing (HPC). %
    % We dedicate this subsection to the \blahblah
    % The computational expense of a simulation is in principle decided by the most time-consuming task of floating point operations and data access. Today, we are not particularly limited by computer arithmetic, but rather memory bandwidth. Furthermore, \blahblah (series vs. parallel)
    % % most time-consuming task  
    % % Computational expense in this contkext is in principle governed by a
    % There are two main parallelisation techniques used in HPC. The Message Passing Interface (MPI) standard functions on parallel computing architectures, such as computer clusters. In lattice simulations, it is sensible to \blahblah}





% \subsection{Numerics}



% \deleteme{\subsection{Parameters}
% ?  
% }




